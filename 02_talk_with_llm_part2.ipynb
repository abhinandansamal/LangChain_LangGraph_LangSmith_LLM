{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "openai_llm_model = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_chat_llm_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts and Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Herzog von Sachsen family was renowned for their lavish lifestyle and extravagant parties. However, there was one member of the family who was notoriously known for his eccentric behavior – King Friedrich August III.\n",
      "\n",
      "King Friedrich August III was known as the \"Mad King\" among the people of Sachsen due to his unpredictable actions and odd habits. He had a peculiar fascination with clocks and would spend hours tinkering with them in his palace. But what made him truly unique was his love for animals.\n",
      "\n",
      "The king had a private zoo in his palace, filled with exotic animals from all around the world. However, his favorite animal was a chimpanzee named Hans, whom he had raised since birth. Hans was treated like a member of the royal family, with a personal servant to cater to his every need.\n",
      "\n",
      "But what made Hans truly special was his ability to mimic human behavior. The king had taught him how to walk, talk, and even dance. Hans became the talk of the town, and people would often come to the palace just to see the chimpanzee's antics.\n",
      "\n",
      "One day, during a royal ball, King Friedrich August III surprised his guests by having Hans perform a waltz with him. The guests were mesmerized by the sight of the king and his chimpanzee dancing\n"
     ]
    }
   ],
   "source": [
    "# for completion model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} story about {topic}.\"\n",
    ")\n",
    "\n",
    "# Pass the input to the prompt (any input that matches the template)\n",
    "llmModelPrompt = prompt_template.format(\n",
    "    adjective=\"curious\",\n",
    "    topic=\"The Herzog von Sachsen king Family\"\n",
    ")\n",
    "\n",
    "# Call the completion model\n",
    "res = openai_llm_model.invoke(llmModelPrompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, knowledge of Natural Language Processing (NLP) and Deep Learning are beneficial when learning about Generative AI. Generative AI involves creating models that can generate new, realistic data based on patterns learned from existing data. NLP and Deep Learning techniques are often used in Generative AI to generate text, images, music, and more. Understanding NLP can help with text generation tasks, while knowledge of Deep Learning can help with building and training complex generative models such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). Having a solid foundation in NLP and Deep Learning concepts will definitely be advantageous when diving into the field of Generative AI.\n"
     ]
    }
   ],
   "source": [
    "# for chat completion model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a chat template\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an {profession} expert on {topic}.\"),\n",
    "        (\"user\", \"Hello, Mr. {profession} expert, can you please answer a question?\"),\n",
    "        (\"ai\", \"Sure! What is your question?\"),\n",
    "        (\"user\", \"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pass the input to the prompt (any input that matches the template) (Few-Shot Prompting)\n",
    "messages = chat_template.format_messages(\n",
    "    profession=\"IT\",\n",
    "    topic=\"Artificial Intelligence\",\n",
    "    user_input=\"Are the knowledge of NLP & Deep Learning required to learn Generative AI?\"\n",
    ")\n",
    "\n",
    "# Call the chat completion model\n",
    "response = openai_chat_llm_model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "# Englsih to Spanish translation example\n",
    "# This example is a few-shot prompt for translating English to Spanish.\n",
    "# The example consists of a list of dictionaries, each containing an input and output pair.\n",
    "# The input is an English phrase, and the output is its Spanish translation.\n",
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"adios!\"}\n",
    "]\n",
    "\n",
    "# Create a few-shot prompt template\n",
    "# The example prompt is defined using the ChatPromptTemplate class.\n",
    "# It consists of a user message with the input and an AI message with the output.\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# The example prompt is then used to create a FewShotChatMessagePromptTemplate.\n",
    "# This template takes the example prompt and the list of examples to create a few-shot prompt.\n",
    "# The example prompt is used to format the examples into a few-shot format.\n",
    "# The examples are passed to the example prompt to create a formatted example.\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "\n",
    "# The final prompt is created by combining the system message, the few-shot prompt, and the user input.\n",
    "# The system message instructs the model to act as an English-Spanish translator.\n",
    "# The user input is passed to the final prompt for translation.\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cómo estás?\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"adios!\"}\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# The final prompt is then used to create a chain with the chat model.\n",
    "# The chain takes the final prompt and the chat model as input.\n",
    "chain = final_prompt | openai_chat_llm_model # LangChain Expression Language (LCL) syntax\n",
    "\n",
    "# The chain is then used to invoke the model with the user input.\n",
    "res = chain.invoke({\"input\": \"How are you?\"})\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/introduction/\n",
    "\n",
    "https://python.langchain.com/docs/concepts/lcel/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "* Used to reformat the response of the large language model.\n",
    "* We can reformat the text in any kind of format, whether it's a json format or txt format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Russia'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "json_prompt = PromptTemplate.from_template(\n",
    "    \"Return a JSON object with an `answer` key that answers the following question: {question}\"\n",
    ")\n",
    "\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "# The JSON chain is created by combining the prompt, the model, and the output parser.\n",
    "# The chain takes the JSON prompt, the OpenAI model, and the JSON parser as input.\n",
    "json_chain = json_prompt | openai_llm_model | json_parser\n",
    "\n",
    "# The chain is then used to invoke the model with the user input.\n",
    "# The input is a question, and the output is a JSON object with the answer.\n",
    "res = json_chain.invoke({\"question\": \"What is the biggest country in the world?\"})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally, you can use Pydantic to define a custom output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pydantic Object with the desired output format\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}\n"
     ]
    }
   ],
   "source": [
    "# Define the parser referring the Pydantic Object\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# Add the parser format instructions to the prompt definition\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Create a chain with the prompt and the parser\n",
    "chain = prompt | openai_chat_llm_model | parser\n",
    "\n",
    "res = chain.invoke({\"query\": \"Tell me a joke.\"})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
