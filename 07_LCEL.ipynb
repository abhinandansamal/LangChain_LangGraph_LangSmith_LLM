{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e09a07",
   "metadata": {},
   "source": [
    "## Runnable Interface\n",
    "* The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as language models, output parsers, retrievers, compiled LangGraph graphs and more.\n",
    "\n",
    "The Runnable way defines a standard interface that allows a Runnable component to be:\n",
    "* `Invoked`: A single input is transformed into an output.\n",
    "* `Batched`: Multiple inputs are efficiently transformed into outputs.\n",
    "* `Streamed`: Outputs are streamed as they are produced.\n",
    "* `Inspected`: Schematic information about Runnable's input, output, and configuration can be accessed.\n",
    "* `Composed`: Multiple Runnables can be composed to work together using the LangChain Expression Language (LCEL) to create complex pipelines.\n",
    "\n",
    "Review the [LCEL Cheatsheet](https://python.langchain.com/docs/how_to/lcel_cheatsheet/) for some common patterns that involve the Runnable interface and LCEL expressions.\n",
    "\n",
    "**Optimized parallel execution (batch):**\n",
    "\n",
    "* LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allows you to process multiple inputs in parallel.\n",
    "* Using these methods can significantly improve performance when needing to process multiple independent inputs, as the processing can be done in parallel instead of sequentially.\n",
    "\n",
    "The two batching options are:\n",
    "* `batch`: Process multiple inputs in parallel, returning results in the same order as the inputs.\n",
    "* `batch_as_completed`: Process multiple inputs in parallel, returning results as they complete. Results may arrive out of order, but each includes the input index for matching.\n",
    "\n",
    "**Streaming APIs:**\n",
    "\n",
    "* Streaming is critical in making applications based on LLMs feel responsive to end-users. Runnables expose the following 3 streaming APIs:\n",
    "\n",
    "1. sync stream and async astream: yields the output a Runnable as it is generated.\n",
    "\n",
    "2. The async `astream_events`: a more advanced streaming API that allows streaming intermediate steps and final output.\n",
    "\n",
    "3. The legacy async `astream_log`: a legacy streaming API that streams intermediate steps and final output.\n",
    "\n",
    "Refer the [Streaming Conceptual Guide](https://python.langchain.com/docs/concepts/streaming/) for more details on how to stream in LangChain.\n",
    "\n",
    "**Input and Output types:**\n",
    "\n",
    "* Every Runnable is characterized by an input and output type. These input and output types can be any Python object, and are defined by the Runnable itself.\n",
    "* Runnable methods that result in the execution of the Runnable (e.g., `invoke`, `batch`, `stream`, `astream_events`) work with these input and output types.\n",
    "    * invoke: Accepts an input and returns an output.\n",
    "    * batch: Accepts a list of inputs and returns a list of outputs.\n",
    "    * stream: Accepts an input and returns a generator that yields outputs.\n",
    "\n",
    "The input type and output type vary by component:\n",
    "\n",
    "\n",
    "Component | Input Type | Output Type\n",
    "----------|------------|---------------\n",
    "Prompt | dictionary | PromptValue\n",
    "ChatModel | a string, list of chat messages or a PromptValue | ChatMessage\n",
    "LLM | a string, list of chat messages or a PromptValue | String\n",
    "OutputParser | the output of an LLM or ChatModel | Depends on the parser\n",
    "Retriever | a string | List of Documents\n",
    "Tool | a string or dictionary, depending on the tool | Depends on the tool \n",
    "\n",
    "For more information about `Runnable`, refer: https://python.langchain.com/docs/concepts/runnables/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97df45",
   "metadata": {},
   "source": [
    "## Simple Chain\n",
    "\n",
    "* It performs several actions in a particular order.\n",
    "* Chains are sequences of actions with input or output data.\n",
    "* Since the emergence of LCEL, LangChain is favouring LCEL chains over Traditional (Legacy) built-in Chains, but these are still maintained and frequently used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef46725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe264f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel = ChatOpenAI(model = \"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faca2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a curious fact about {politician}.\")\n",
    "\n",
    "chain = prompt | chatModel | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd589a5",
   "metadata": {},
   "source": [
    "**Output Parser** automatically extracts the response content and return it. Otherwise the response comes in the form of AIMessage(content=\".....\"). **StrOutputParser** returns the response content in the form of string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f921ee81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barack Obama is known for being left-handed, making him the sixth U.S. president to be left-handed.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"politician\": \"Barack Obama\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf7ee10",
   "metadata": {},
   "source": [
    "## LCEL\n",
    "\n",
    "* LCEL has become the backbone of the newest versions of LangChain.\n",
    "* Traditional chains (https://python.langchain.com/v0.1/docs/modules/chains/) are still supported, but treated as \"Legacy\" and have less functionality than the new LCEL chains.\n",
    "\n",
    "**Main goals of LCEL**:\n",
    "\n",
    "* Make it easy to build chains in a compact way.\n",
    "* Support advanced LangChain functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049d7d9",
   "metadata": {},
   "source": [
    "## Legacy Chain Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce147688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-d4c6792a9a50>:5: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  traditional_chain = LLMChain(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Diego Maradona, one of the greatest football players of all time, has a species of butterfly named after him. The \"Diaethria maradonae\" butterfly was discovered in the rainforests of Peru in 1999 and was named in honor of Maradona\\'s famous \"Hand of God\" goal in the 1986 FIFA World Cup.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a curious fact about {soccer_player}.\")\n",
    "\n",
    "traditional_chain = LLMChain(\n",
    "    llm=chatModel,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "traditional_chain.predict(soccer_player=\"Maradona\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e70deab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One curious fact about Diego Maradona is that he has a species of slug named after him. The newly discovered species of slug, known as the 'Bieler's Maradona slug' was named in honor of the football legend due to its agility and speed, similar to Maradona's playing style on the field.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performing above operation using LCEL\n",
    "chain = prompt | chatModel | StrOutputParser()\n",
    "chain.invoke({\"soccer_player\": \"Maradona\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ddd80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One curious fact about Ronaldo is that he is known for his incredible work ethic and dedication to training. He reportedly spends hours in the gym and on the training field, constantly striving to improve his skills and maintain his peak physical condition. This level of commitment has played a significant role in his success as one of the greatest footballers of all time.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another example\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a curious fact about {soccer_player}.\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | chatModel | output_parser\n",
    "\n",
    "chain.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a497c0",
   "metadata": {},
   "source": [
    "## Runnable execution order in a LCEL chain\n",
    "\n",
    "User's Input -> Prompt Template -> LLM Model -> Output Parser -> Final Output\n",
    "\n",
    "In the above case,\n",
    "\n",
    "User's Input = \"soccer_player\": \"Ronaldo\"\n",
    "\n",
    "Prompt Template = \"Tell me a curious fact about Ronaldo\"\n",
    "\n",
    "The output of LLM Model (in this case it is \"chatModel\") = AIMessage(content=\"One curious fact about Ronaldo is that he is known for ...\")\n",
    "\n",
    "The output of Output Parser = 'One curious fact about Ronaldo is that he is known for ...' , which is the Final Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f0037f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ronaldo is known for his incredible work ethic and dedication to his training regimen. He reportedly has a personal chef who travels with him to ensure he maintains a strict diet and nutrition plan, and he also reportedly sleeps in a hyperbaric chamber to aid in muscle recovery and overall performance."
     ]
    }
   ],
   "source": [
    "# Runnable execution alternatives i.e., stream, batch etc.\n",
    "for s in chain.stream({\"soccer_player\": \"Ronaldo\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9422c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ronaldo is known for his incredible work ethic and dedication to his fitness, but one curious fact about him is that he reportedly sleeps for an average of 5 naps per day, each lasting around 90 minutes. This unique sleep schedule is said to help him maintain his high energy levels and performance on the field.',\n",
       " 'Lionel Messi holds the record for the most goals scored in a calendar year, with 91 goals in 2012.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"soccer_player\": \"Ronaldo\"}, {\"soccer_player\": \"Messi\"}])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
